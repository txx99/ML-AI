{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Unsupervised Learning II - Dimensionality Reduction\n",
    "\n",
    "In this lab, we will cover the following topics:\n",
    "1. Dimensionality reduction techniques:\n",
    "   * PCA\n",
    "   * t-SNE\n",
    "   * UMAP\n",
    "2. Visualization and interpretation\n",
    "\n",
    "Each section includes basic implementation and questions for further exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dimensionality Reduction\n",
    "\n",
    "### 1.1 Principal Component Analysis (PCA)\n",
    "\n",
    "We will start by implementing PCA for dimensionality reduction. We will also explore different parameters and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.colorbar(label='Class')\n",
    "plt.show()\n",
    "\n",
    "# Scree plot\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for Exploration\n",
    "\n",
    "1. How does changing the number of principal components (`n_components`) affect the explained variance ratio?\n",
    "2. What happens to the visualization when you use more or fewer principal components?\n",
    "3. How does the choice of dataset affect the PCA results and their interpretation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "Next, we will implement t-SNE for dimensionality reduction. We will also explore different parameters and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Perform t-SNE with different perplexity values\n",
    "perplexities = [5, 30, 50]\n",
    "fig, axes = plt.subplots(1, len(perplexities), figsize=(18, 6))\n",
    "\n",
    "for i, perplexity in enumerate(perplexities):\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X)\n",
    "    axes[i].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')\n",
    "    axes[i].set_title(f't-SNE (perplexity={perplexity})')\n",
    "    axes[i].set_xlabel('t-SNE Component 1')\n",
    "    axes[i].set_ylabel('t-SNE Component 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for Exploration\n",
    "\n",
    "1. How does changing the perplexity parameter affect the t-SNE visualization?\n",
    "2. What happens to the visualization when you change the learning rate?\n",
    "3. How does the choice of dataset affect the t-SNE results and their interpretation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Uniform Manifold Approximation and Projection (UMAP)\n",
    "\n",
    "Finally, we will implement UMAP for dimensionality reduction. We will also explore different parameters and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "# Perform UMAP with different n_neighbors values\n",
    "n_neighbors_values = [5, 15, 50]\n",
    "fig, axes = plt.subplots(1, len(n_neighbors_values), figsize=(18, 6))\n",
    "\n",
    "for i, n_neighbors in enumerate(n_neighbors_values):\n",
    "    umap_reducer = umap.UMAP(n_components=2, n_neighbors=n_neighbors, random_state=42)\n",
    "    X_umap = umap_reducer.fit_transform(X)\n",
    "    axes[i].scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='viridis')\n",
    "    axes[i].set_title(f'UMAP (n_neighbors={n_neighbors})')\n",
    "    axes[i].set_xlabel('UMAP Component 1')\n",
    "    axes[i].set_ylabel('UMAP Component 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for Exploration\n",
    "\n",
    "1. How does changing the number of neighbors (`n_neighbors`) affect the UMAP visualization?\n",
    "2. What happens to the visualization when you change the minimum distance (`min_dist`)?\n",
    "3. How does the choice of dataset affect the UMAP results and their interpretation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualization and Interpretation\n",
    "\n",
    "We will visualize and interpret the results of different dimensionality reduction techniques. We will also explore different visualization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Perform t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Perform UMAP\n",
    "umap_reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "X_umap = umap_reducer.fit_transform(X)\n",
    "\n",
    "# Plot the results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n",
    "axes[0].set_title('PCA')\n",
    "axes[0].set_xlabel('Component 1')\n",
    "axes[0].set_ylabel('Component 2')\n",
    "\n",
    "axes[1].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis')\n",
    "axes[1].set_title('t-SNE')\n",
    "axes[1].set_xlabel('Component 1')\n",
    "axes[1].set_ylabel('Component 2')\n",
    "\n",
    "axes[2].scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='viridis')\n",
    "axes[2].set_title('UMAP')\n",
    "axes[2].set_xlabel('Component 1')\n",
    "axes[2].set_ylabel('Component 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions for Exploration\n",
    "\n",
    "1. How do the visualizations of PCA, t-SNE, and UMAP compare?\n",
    "2. What are the strengths and weaknesses of each dimensionality reduction technique?\n",
    "3. How does the choice of parameters affect the visualization and interpretation of each technique?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "1. Apply clustering algorithms (e.g., k-Means) on the reduced dimensions obtained from PCA, t-SNE, and UMAP.\n",
    "    * How do the clustering results compare when using different dimensionality reduction techniques?\n",
    "2. Use quantitative metrics (e.g., trustworthiness, continuity) to evaluate the quality of the dimensionality reduction.\n",
    "    * How do PCA, t-SNE, and UMAP compare based on these metrics?\n",
    "Handling High-Dimensional Data\n",
    "3. Use PCA to reduce noise in a dataset by retaining only the top principal components.\n",
    "    * How does this noise reduction affect the performance of downstream tasks (e.g., classification, clustering)?\n",
    "4. Explore other dimensionality reduction techniques such as Independent Component Analysis (ICA), Linear Discriminant Analysis (LDA), and Isomap.\n",
    "    * How do these techniques compare to PCA, t-SNE, and UMAP in terms of visualization and interpretation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
